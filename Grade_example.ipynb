{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 900 samples\n",
      "Epoch 1/200\n",
      "900/900 - 1s - loss: 0.0616\n",
      "Epoch 2/200\n",
      "900/900 - 0s - loss: 0.0364\n",
      "Epoch 3/200\n",
      "900/900 - 0s - loss: 0.0360\n",
      "Epoch 4/200\n",
      "900/900 - 0s - loss: 0.0354\n",
      "Epoch 5/200\n",
      "900/900 - 0s - loss: 0.0349\n",
      "Epoch 6/200\n",
      "900/900 - 0s - loss: 0.0345\n",
      "Epoch 7/200\n",
      "900/900 - 0s - loss: 0.0339\n",
      "Epoch 8/200\n",
      "900/900 - 0s - loss: 0.0331\n",
      "Epoch 9/200\n",
      "900/900 - 0s - loss: 0.0326\n",
      "Epoch 10/200\n",
      "900/900 - 0s - loss: 0.0320\n",
      "Epoch 11/200\n",
      "900/900 - 0s - loss: 0.0310\n",
      "Epoch 12/200\n",
      "900/900 - 0s - loss: 0.0307\n",
      "Epoch 13/200\n",
      "900/900 - 0s - loss: 0.0296\n",
      "Epoch 14/200\n",
      "900/900 - 0s - loss: 0.0285\n",
      "Epoch 15/200\n",
      "900/900 - 0s - loss: 0.0277\n",
      "Epoch 16/200\n",
      "900/900 - 0s - loss: 0.0266\n",
      "Epoch 17/200\n",
      "900/900 - 0s - loss: 0.0255\n",
      "Epoch 18/200\n",
      "900/900 - 0s - loss: 0.0246\n",
      "Epoch 19/200\n",
      "900/900 - 0s - loss: 0.0239\n",
      "Epoch 20/200\n",
      "900/900 - 0s - loss: 0.0228\n",
      "Epoch 21/200\n",
      "900/900 - 0s - loss: 0.0219\n",
      "Epoch 22/200\n",
      "900/900 - 0s - loss: 0.0211\n",
      "Epoch 23/200\n",
      "900/900 - 0s - loss: 0.0203\n",
      "Epoch 24/200\n",
      "900/900 - 0s - loss: 0.0197\n",
      "Epoch 25/200\n",
      "900/900 - 0s - loss: 0.0188\n",
      "Epoch 26/200\n",
      "900/900 - 0s - loss: 0.0181\n",
      "Epoch 27/200\n",
      "900/900 - 0s - loss: 0.0176\n",
      "Epoch 28/200\n",
      "900/900 - 0s - loss: 0.0170\n",
      "Epoch 29/200\n",
      "900/900 - 0s - loss: 0.0164\n",
      "Epoch 30/200\n",
      "900/900 - 0s - loss: 0.0162\n",
      "Epoch 31/200\n",
      "900/900 - 0s - loss: 0.0156\n",
      "Epoch 32/200\n",
      "900/900 - 0s - loss: 0.0150\n",
      "Epoch 33/200\n",
      "900/900 - 0s - loss: 0.0145\n",
      "Epoch 34/200\n",
      "900/900 - 0s - loss: 0.0141\n",
      "Epoch 35/200\n",
      "900/900 - 0s - loss: 0.0137\n",
      "Epoch 36/200\n",
      "900/900 - 0s - loss: 0.0136\n",
      "Epoch 37/200\n",
      "900/900 - 0s - loss: 0.0130\n",
      "Epoch 38/200\n",
      "900/900 - 0s - loss: 0.0127\n",
      "Epoch 39/200\n",
      "900/900 - 0s - loss: 0.0122\n",
      "Epoch 40/200\n",
      "900/900 - 0s - loss: 0.0119\n",
      "Epoch 41/200\n",
      "900/900 - 0s - loss: 0.0116\n",
      "Epoch 42/200\n",
      "900/900 - 0s - loss: 0.0112\n",
      "Epoch 43/200\n",
      "900/900 - 0s - loss: 0.0109\n",
      "Epoch 44/200\n",
      "900/900 - 0s - loss: 0.0108\n",
      "Epoch 45/200\n",
      "900/900 - 0s - loss: 0.0107\n",
      "Epoch 46/200\n",
      "900/900 - 0s - loss: 0.0100\n",
      "Epoch 47/200\n",
      "900/900 - 0s - loss: 0.0098\n",
      "Epoch 48/200\n",
      "900/900 - 0s - loss: 0.0095\n",
      "Epoch 49/200\n",
      "900/900 - 0s - loss: 0.0092\n",
      "Epoch 50/200\n",
      "900/900 - 0s - loss: 0.0090\n",
      "Epoch 51/200\n",
      "900/900 - 0s - loss: 0.0087\n",
      "Epoch 52/200\n",
      "900/900 - 0s - loss: 0.0085\n",
      "Epoch 53/200\n",
      "900/900 - 0s - loss: 0.0084\n",
      "Epoch 54/200\n",
      "900/900 - 0s - loss: 0.0082\n",
      "Epoch 55/200\n",
      "900/900 - 0s - loss: 0.0080\n",
      "Epoch 56/200\n",
      "900/900 - 0s - loss: 0.0078\n",
      "Epoch 57/200\n",
      "900/900 - 0s - loss: 0.0078\n",
      "Epoch 58/200\n",
      "900/900 - 0s - loss: 0.0075\n",
      "Epoch 59/200\n",
      "900/900 - 0s - loss: 0.0073\n",
      "Epoch 60/200\n",
      "900/900 - 0s - loss: 0.0072\n",
      "Epoch 61/200\n",
      "900/900 - 0s - loss: 0.0071\n",
      "Epoch 62/200\n",
      "900/900 - 0s - loss: 0.0069\n",
      "Epoch 63/200\n",
      "900/900 - 0s - loss: 0.0069\n",
      "Epoch 64/200\n",
      "900/900 - 0s - loss: 0.0068\n",
      "Epoch 65/200\n",
      "900/900 - 0s - loss: 0.0067\n",
      "Epoch 66/200\n",
      "900/900 - 0s - loss: 0.0066\n",
      "Epoch 67/200\n",
      "900/900 - 0s - loss: 0.0066\n",
      "Epoch 68/200\n",
      "900/900 - 0s - loss: 0.0065\n",
      "Epoch 69/200\n",
      "900/900 - 0s - loss: 0.0065\n",
      "Epoch 70/200\n",
      "900/900 - 0s - loss: 0.0064\n",
      "Epoch 71/200\n",
      "900/900 - 0s - loss: 0.0064\n",
      "Epoch 72/200\n",
      "900/900 - 0s - loss: 0.0064\n",
      "Epoch 73/200\n",
      "900/900 - 0s - loss: 0.0063\n",
      "Epoch 74/200\n",
      "900/900 - 0s - loss: 0.0062\n",
      "Epoch 75/200\n",
      "900/900 - 0s - loss: 0.0062\n",
      "Epoch 76/200\n",
      "900/900 - 0s - loss: 0.0062\n",
      "Epoch 77/200\n",
      "900/900 - 0s - loss: 0.0063\n",
      "Epoch 78/200\n",
      "900/900 - 0s - loss: 0.0061\n",
      "Epoch 79/200\n",
      "900/900 - 0s - loss: 0.0061\n",
      "Epoch 80/200\n",
      "900/900 - 0s - loss: 0.0062\n",
      "Epoch 81/200\n",
      "900/900 - 0s - loss: 0.0061\n",
      "Epoch 82/200\n",
      "900/900 - 0s - loss: 0.0061\n",
      "Epoch 83/200\n",
      "900/900 - 0s - loss: 0.0061\n",
      "Epoch 84/200\n",
      "900/900 - 0s - loss: 0.0061\n",
      "Epoch 85/200\n",
      "900/900 - 0s - loss: 0.0061\n",
      "Epoch 86/200\n",
      "900/900 - 0s - loss: 0.0060\n",
      "Epoch 87/200\n",
      "900/900 - 0s - loss: 0.0061\n",
      "Epoch 88/200\n",
      "900/900 - 0s - loss: 0.0061\n",
      "Epoch 89/200\n",
      "900/900 - 0s - loss: 0.0062\n",
      "Epoch 90/200\n",
      "900/900 - 0s - loss: 0.0061\n",
      "Epoch 91/200\n",
      "900/900 - 0s - loss: 0.0060\n",
      "Epoch 92/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 93/200\n",
      "900/900 - 0s - loss: 0.0060\n",
      "Epoch 94/200\n",
      "900/900 - 0s - loss: 0.0061\n",
      "Epoch 95/200\n",
      "900/900 - 0s - loss: 0.0060\n",
      "Epoch 96/200\n",
      "900/900 - 0s - loss: 0.0060\n",
      "Epoch 97/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 98/200\n",
      "900/900 - 0s - loss: 0.0060\n",
      "Epoch 99/200\n",
      "900/900 - 0s - loss: 0.0060\n",
      "Epoch 100/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 101/200\n",
      "900/900 - 0s - loss: 0.0061\n",
      "Epoch 102/200\n",
      "900/900 - 0s - loss: 0.0060\n",
      "Epoch 103/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 104/200\n",
      "900/900 - 0s - loss: 0.0060\n",
      "Epoch 105/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 106/200\n",
      "900/900 - 0s - loss: 0.0060\n",
      "Epoch 107/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 108/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 109/200\n",
      "900/900 - 0s - loss: 0.0061\n",
      "Epoch 110/200\n",
      "900/900 - 0s - loss: 0.0060\n",
      "Epoch 111/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 112/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 113/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 114/200\n",
      "900/900 - 0s - loss: 0.0060\n",
      "Epoch 115/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 116/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 117/200\n",
      "900/900 - 0s - loss: 0.0060\n",
      "Epoch 118/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 119/200\n",
      "900/900 - 0s - loss: 0.0060\n",
      "Epoch 120/200\n",
      "900/900 - 0s - loss: 0.0060\n",
      "Epoch 121/200\n",
      "900/900 - 0s - loss: 0.0060\n",
      "Epoch 122/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 123/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 124/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 125/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 126/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 127/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 128/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 129/200\n",
      "900/900 - 0s - loss: 0.0060\n",
      "Epoch 130/200\n",
      "900/900 - 0s - loss: 0.0060\n",
      "Epoch 131/200\n",
      "900/900 - 0s - loss: 0.0060\n",
      "Epoch 132/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 133/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 134/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 135/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 136/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 137/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 138/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 139/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 140/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 141/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 142/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 143/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 144/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 145/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 146/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 147/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 148/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 149/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 150/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 151/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 152/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 153/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 154/200\n",
      "900/900 - 0s - loss: 0.0060\n",
      "Epoch 155/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 156/200\n",
      "900/900 - 0s - loss: 0.0060\n",
      "Epoch 157/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 158/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 159/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 160/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 161/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 162/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 163/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 164/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 165/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 166/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 167/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 168/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 169/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 170/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 171/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 172/200\n",
      "900/900 - 0s - loss: 0.0060\n",
      "Epoch 173/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 174/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 175/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 176/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 177/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 178/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 179/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 180/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 181/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 182/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 183/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 184/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 185/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 186/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 187/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 188/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 189/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 190/200\n",
      "900/900 - 0s - loss: 0.0059\n",
      "Epoch 191/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 192/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 193/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 194/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 195/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 196/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 197/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "Epoch 198/200\n",
      "900/900 - 0s - loss: 0.0057\n",
      "Epoch 199/200\n",
      "900/900 - 0s - loss: 0.0057\n",
      "Epoch 200/200\n",
      "900/900 - 0s - loss: 0.0058\n",
      "3.236449701126438\n",
      "[0.         0.         0.02819678 0.         0.00733949 0.00180974\n",
      " 0.962654  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16127\\Anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.tree.export module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.tree. Anything that cannot be imported from sklearn.tree is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      studytime  traveltime  failures  freetime  absences  G1  G2  G3\n",
      "0             2           2         0         3         6   5   6   6\n",
      "1             2           1         0         3         4   5   5   6\n",
      "2             2           1         3         3        10   7   8  10\n",
      "3             3           1         0         2         2  15  14  15\n",
      "4             2           1         0         3         4   6  10  10\n",
      "...         ...         ...       ...       ...       ...  ..  ..  ..\n",
      "1039          3           1         1         4         4  10  11  10\n",
      "1040          2           1         0         3         4  15  15  16\n",
      "1041          2           2         0         1         6  11  12   9\n",
      "1042          1           2         0         4         6  10  10  10\n",
      "1043          1           3         0         4         4  10  11  11\n",
      "\n",
      "[1044 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.svm import SVR\n",
    "import csv\n",
    "#setting\n",
    "datafile = 'studentperform.csv'\n",
    "studentmodel = 'studentmodel.h5'\n",
    "batch_size = 10\n",
    "hidden_neuron = 10\n",
    "trainsize = 900\n",
    "iterasi = 200\n",
    "def generatemodel(totvar):\n",
    "# create and fit the LSTM network\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(3, batch_input_shape=(batch_size, totvar), activation='sigmoid'))\n",
    "    model.add(tf.keras.layers.Dense(hidden_neuron, activation='sigmoid'))\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "#read data\n",
    "alldata = np.genfromtxt(datafile,delimiter=',')[1:]\n",
    "#separate between training and test\n",
    "trainparam = alldata[:900, :-1]\n",
    "trainlabel = alldata[:900, -1]\n",
    "testparam = alldata[900:, :-1]\n",
    "testlabel = alldata[900:, -1]\n",
    "trainparam = trainparam[len(trainparam)%10:]\n",
    "trainlabel = trainlabel[len(trainlabel)%10:]\n",
    "testparam = testparam[len(testparam)%10:]\n",
    "testlabel = testlabel[len(testlabel)%10:]\n",
    "###############\n",
    "#normalization#\n",
    "###############\n",
    "trainparamnorm = np.zeros(np.shape(trainparam))\n",
    "trainlabelnorm = np.zeros(np.shape(trainlabel))\n",
    "testparamnorm = np.zeros(np.shape(testparam))\n",
    "testlabelnorm = np.zeros(np.shape(testlabel))\n",
    "\n",
    "for i in range(len(trainparam[0])-2):\n",
    "    trainparamnorm[:,i] = (trainparam[:,i] - np.min(trainparam[:,i])) / (np.max(trainparam[:,i]) - np.min(trainparam[:,i]))\n",
    "    testparamnorm[:,i] = (testparam[:,i] - np.min(trainparam[:,i])) / (np.max(trainparam[:,i]) - np.min(trainparam[:,i]))\n",
    "for i in range(2):\n",
    "    trainparamnorm[:,-2+i] = (trainparam[:,-2+i] - 0.0) / (20.0 - 0.0)\n",
    "    testparamnorm[:,-2+i] = (testparam[:,-2+i] - 0.0) / (20.0 - 0.0)\n",
    "#for label\n",
    "trainlabelnorm = (trainlabel - np.min(trainlabel)) / (np.max(trainlabel) - np.min(trainlabel))\n",
    "testlabelnorm = (testlabel - np.min(trainlabel)) / (np.max(trainlabel) - np.min(trainlabel))\n",
    "######################\n",
    "#build and save model#\n",
    "######################\n",
    "mod = generatemodel(len(trainparamnorm[0]))\n",
    "mod.fit(trainparamnorm, trainlabelnorm, epochs=iterasi, batch_size=batch_size, verbose=2, shuffle=True)\n",
    "#save trained model\n",
    "mod.save(studentmodel)\n",
    "G3pred = mod.predict(testparamnorm, batch_size=batch_size)\n",
    "G3real = G3pred*20.0\n",
    "\n",
    "errreal = mean_squared_error(testlabel, G3real)\n",
    "\n",
    "print(errreal)\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "import pydot\n",
    "import graphviz\n",
    "from sklearn.tree.export import export_graphviz\n",
    "data = pd.read_csv('studentperform.csv')\n",
    "clf = tree.DecisionTreeClassifier(criterion=\"gini\")\n",
    "y = np.zeros(G3real.shape[0])\n",
    "for i in range(y.shape[0]):\n",
    "    if G3real[i]>=np.mean(G3real):\n",
    "        y[i]=1\n",
    "    else:\n",
    "        y[i]=0\n",
    "clf.fit(testparamnorm,y)\n",
    "print(clf.feature_importances_)\n",
    "dot_data = tree.export_graphviz(clf, out_file=None)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render(\"Grade\")\n",
    "dot_data = export_graphviz(clf, out_file=None,feature_names=data.columns[1:8],\n",
    "                           filled=True, rounded=True,\n",
    "                           special_characters=True)\n",
    "graph = graphviz.Source(dot_data)\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                      y   R-squared (uncentered):                   0.984\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.984\n",
      "Method:                 Least Squares   F-statistic:                              7794.\n",
      "Date:                Fri, 18 Dec 2020   Prob (F-statistic):                        0.00\n",
      "Time:                        16:40:09   Log-Likelihood:                          1033.9\n",
      "No. Observations:                 900   AIC:                                     -2054.\n",
      "Df Residuals:                     893   BIC:                                     -2020.\n",
      "Df Model:                           7                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1            -0.0178      0.009     -1.932      0.054      -0.036       0.000\n",
      "x2             0.0180      0.011      1.709      0.088      -0.003       0.039\n",
      "x3            -0.0639      0.012     -5.215      0.000      -0.088      -0.040\n",
      "x4            -0.0113      0.009     -1.213      0.225      -0.030       0.007\n",
      "x5             0.0583      0.029      2.003      0.045       0.001       0.115\n",
      "x6             0.0654      0.033      1.968      0.049       0.000       0.131\n",
      "x7             0.9672      0.031     30.752      0.000       0.906       1.029\n",
      "==============================================================================\n",
      "Omnibus:                      635.144   Durbin-Watson:                   1.787\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            10157.343\n",
      "Skew:                          -3.060   Prob(JB):                         0.00\n",
      "Kurtosis:                      18.278   Cond. No.                         18.8\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] RÂ² is computed without centering (uncentered) since the model does not contain a constant.\n",
      "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "reg = LinearRegression().fit(trainparamnorm, trainlabelnorm)\n",
    "reg.predict(testparamnorm)*20\n",
    "mean_squared_error(testlabel, G3real)\n",
    "est = sm.OLS( trainlabelnorm, trainparamnorm)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True False ... False False  True]\n",
      " [ True  True False ... False False  True]\n",
      " [False False  True ...  True  True False]\n",
      " ...\n",
      " [ True  True False ... False False  True]\n",
      " [False False  True ...  True  True False]\n",
      " [ True  True False ... False False  True]]\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3688959105934453"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.236449701126438\n",
      "[0.         0.         0.         0.         0.00733949 0.03000652\n",
      " 0.962654  ]\n"
     ]
    }
   ],
   "source": [
    "print(errreal)\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "import pydot\n",
    "import graphviz\n",
    "from sklearn.tree.export import export_graphviz\n",
    "data = pd.read_csv('studentperform.csv')\n",
    "clf = tree.DecisionTreeClassifier(criterion=\"gini\")\n",
    "y = np.zeros(G3real.shape[0])\n",
    "for i in range(y.shape[0]):\n",
    "    if G3real[i]>=np.mean(G3real):\n",
    "        y[i]=1\n",
    "    else:\n",
    "        y[i]=0\n",
    "clf.fit(testparamnorm,y)\n",
    "print(clf.feature_importances_)\n",
    "dot_data = tree.export_graphviz(clf, out_file=None)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render(\"Grade\")\n",
    "dot_data = export_graphviz(clf, out_file=None,feature_names=data.columns[1:8],\n",
    "                           filled=True, rounded=True,\n",
    "                           special_characters=True)\n",
    "graph = graphviz.Source(dot_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
